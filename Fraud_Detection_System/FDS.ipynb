{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDS (Fraud Detection System, 이상금융거래탐지시스템)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, gc\n",
    "import multiprocessing\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\n",
    "from tensorflow.contrib.learn import Experiment\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "from tensorflow.contrib.learn.python.learn.utils import (saved_model_export_utils)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.preprocessing import Imputer, RobustScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "OUTDIR='./tmp/trained_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed load\n",
      "Completed load\n",
      "Completed load\n",
      "Completed load\n",
      "Completed load\n"
     ]
    }
   ],
   "source": [
    "train_tr = pd.read_csv('./input/train_transaction.csv', index_col='TransactionID')\n",
    "print(\"Completed load\")\n",
    "train_id = pd.read_csv('./input/train_identity.csv', index_col='TransactionID')\n",
    "print(\"Completed load\")\n",
    "test_tr = pd.read_csv('./input/test_transaction.csv', index_col='TransactionID')\n",
    "print(\"Completed load\")\n",
    "test_id = pd.read_csv('./input/test_identity.csv', index_col='TransactionID')\n",
    "print(\"Completed load\")\n",
    "sub = pd.read_csv('./input/sample_submission.csv', index_col='TransactionID')\n",
    "print(\"Completed load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join by TransactionId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1955.37 MB\n"
     ]
    }
   ],
   "source": [
    "train = pd.merge(train_tr, train_id,\n",
    "                        how='left',\n",
    "                        on='TransactionID')\n",
    "del train_tr, train_id\n",
    "\n",
    "test = pd.merge(test_tr, test_id,\n",
    "                        how='left',\n",
    "                        on='TransactionID')\n",
    "del test_tr, test_id\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test  = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA(Exploratory Data Analysis)\n",
    "* Except for three variables with very large variance, all variables are treated as objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msno.heatmap(train.iloc[:, :5])\n",
    "# msno.matrix(df=train.iloc[:, 61:80], figsize=(20, 20), color=(0.2, 0.3, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraud_ratio_pie(df, s):\n",
    "    ###Test data Fraud ratio pie chart\n",
    "    f, ax = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "    df['isFraud'].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0], shadow=True)\n",
    "    ax[0].set_title('{} Data Pie plot - isFraud'.format(str(s)))\n",
    "    ax[0].set_ylabel('')\n",
    "    sns.countplot('isFraud', data=df, ax=ax[1])\n",
    "    ax[1].set_title('Count plot - isFraud')\n",
    "\n",
    "    plt.show()\n",
    "fraud_ratio_pie(train,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covert_to_object(df):\n",
    "    # covert to object for reuse\n",
    "    df['ProductCD'] = df['ProductCD'].astype('object')\n",
    "    df['P_emaildomain'] = df['P_emaildomain'].astype('object')\n",
    "    df['R_emaildomain'] = df['R_emaildomain'].astype('object')    \n",
    "    df['DeviceType'] = df['DeviceType'].astype('object')\n",
    "    df['DeviceInfo'] = df['DeviceInfo'].astype('object')\n",
    "\n",
    "    card_cols = [c for c in df.columns if 'card' in c]\n",
    "    for col in card_cols:\n",
    "        df[col] = df[col].astype('object')\n",
    "\n",
    "    addres_cols = [c for c in df.columns if 'addr' in c]\n",
    "    for col in addres_cols:\n",
    "            df[col] = df[col].astype('object')\n",
    "\n",
    "    M_cols = [c for c in df.columns if 'M' in c]\n",
    "    for col in M_cols:\n",
    "            df[col] = df[col].astype('object')\n",
    "\n",
    "    C_cols = [c for c in df.columns if 'C' in c]\n",
    "    for col in C_cols:\n",
    "            df[col] = df[col].astype('object')\n",
    "\n",
    "    id_cols = [c for c in df.columns if 'id' in c]\n",
    "    for col in id_cols:\n",
    "            df[col] = df[col].astype('object')\n",
    "            \n",
    "covert_to_object(train)\n",
    "covert_to_object(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n",
    "ax = sns.distplot(train['dist1'].dropna(axis = 0), bins=10, hist=False, ax=ax1)\n",
    "ax1.set_title('dist1 Distribution', fontsize=14)\n",
    "\n",
    "ax = sns.distplot(train['dist2'].dropna(axis = 0), bins=10, hist=False, ax=ax2)\n",
    "ax2.set_title('dist2 Distribution', fontsize=14)\n",
    "\n",
    "ax = sns.distplot(train['TransactionAmt'].dropna(axis = 0), bins=2, hist=False, ax=ax3)\n",
    "ax3.set_title('TransactionAmt Distribution', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_check(df):\n",
    "    missing_values_mean = df[df.columns].isnull().mean()\n",
    "    print (missing_values_mean.head())\n",
    "#     total_cells = np.product(df.shape)\n",
    "#     total_missing = missing_values_count.sum()\n",
    "    print (\"Percentage of Missing data = \", missing_values_mean * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_categorical(df, threshold=100.00): \n",
    "    missing_percentage= df[df.columns].isnull().mean() * 100\n",
    "    columns = missing_percentage[missing_percentage<threshold].index\n",
    "    df = df[columns]\n",
    "    \n",
    "    features_categorical = df.select_dtypes(include = ['object']).columns\n",
    "    df[features_categorical] = df[features_categorical].fillna('-999',inplace=False)\n",
    "    \n",
    "    missing_values_check(df)\n",
    "    \n",
    "    del features_categorical\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_numerical(df, threshold=100.00): \n",
    "    missing_percentage= df[df.columns].isnull().mean() * 100\n",
    "    columns = missing_percentage[missing_percentage<threshold].index\n",
    "    df = df[columns]\n",
    "    \n",
    "    features_dist_transAmt = df[['dist1','dist2','TransactionAmt']].columns\n",
    "    \n",
    "    imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    imputer = imputer.fit(df[features_dist_transAmt])\n",
    "    \n",
    "    df = imputer.transform(df[features_dist_transAmt])\n",
    "    df = pd.DataFrame(df, columns=['dist1','dist2','TransactionAmt'])\n",
    "    \n",
    "    del features_dist_transAmt\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Train data missing = \",(train[train.columns].isnull().sum().sum()/np.product(train.shape)) * 100,\"%\")\n",
    "print (\"Test data missing = \",(test[test.columns].isnull().sum().sum()/np.product(test.shape)) * 100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleansing(df, threshold=100.0): \n",
    "    features_numeric = \\\n",
    "        df.select_dtypes(include = ['int8', 'int32', 'int64','float16','float64']).columns\n",
    "    col_1 = [c for c in features_numeric if c in ['dist1','dist2','TransactionAmt']]\n",
    "    col_2 = [c for c in features_numeric if c not in col_1] #V1~, M1~, D1~\n",
    "\n",
    "    s = df.index\n",
    "    df_clean_col_1 = missing_values_numerical(df[col_1], threshold)\n",
    "    df_clean_col_1 = df_clean_col_1.set_index([s])\n",
    "\n",
    "    df_clean_col_2 = df[col_2].fillna(-999)\n",
    "    df_clean_col_2 = df_clean_col_2.set_index([s])\n",
    "\n",
    "    #Numerical cleanse\n",
    "    df_clean_N = \\\n",
    "        df_clean_col_1.merge(df_clean_col_2, left_on='TransactionID', right_on='TransactionID', how='left')\n",
    "    del df_clean_col_1, df_clean_col_2\n",
    "    #missing_values_check(df_clean_N)\n",
    "    \n",
    "    features_category = df.select_dtypes(include = ['object']).columns\n",
    "    df_clean_C = missing_values_categorical(train[features_category], threshold)\n",
    "    #train_clean_C['card1'] = train_clean_C['card1'].astype('object')\n",
    "    #missing_values_check(train_clean_C)\n",
    "    \n",
    "    #Categorical cleanse\n",
    "    df_clean = \\\n",
    "     df_clean_N.merge(df_clean_C, left_on='TransactionID', right_on='TransactionID', how='left')\n",
    "    \n",
    "    del df_clean_N, df_clean_C\n",
    "    \n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = data_cleansing(train,100.0).copy()\n",
    "test_clean  = data_cleansing(test,100.0).copy()\n",
    "\n",
    "# for mismatch name of columns (eg. id_37y)\n",
    "test_clean = test_clean.fillna(-999)\n",
    "\n",
    "print(train_clean.isnull().sum())\n",
    "print(test_clean.isnull().sum())\n",
    "\n",
    "del train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing_values_check(train_clean_C)\n",
    "test_clean.id_38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransactionDT guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '2018-01-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "train_clean['TransactionDT'] = \\\n",
    "    train_clean['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "# print(train_clean.head())\n",
    "# print(train_clean.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "* dist1, dist2, TransactionAmt\n",
    "* downscale with log, plus 1 for preventing log error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before scaling\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 10))\n",
    "ax = sns.distplot(train_clean['dist1'].dropna(axis = 0), bins=10, hist=False, ax=ax1)\n",
    "ax1.set_title('dist1 Distribution', fontsize=14)\n",
    "\n",
    "ax = sns.distplot(train_clean['dist2'].dropna(axis = 0), bins=10, hist=False, ax=ax2)\n",
    "ax2.set_title('dist2 Distribution', fontsize=14)\n",
    "\n",
    "ax = sns.distplot(train_clean['TransactionAmt'].dropna(axis = 0), bins=2, hist=False, ax=ax3)\n",
    "ax3.set_title('TransactionAmt Distribution', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The overall scaling results are similar, \n",
    "but RobustScaler is less prone to outliers.\n",
    "\"\"\"\n",
    "def feature_scaling(df_clean): \n",
    "\n",
    "     #downscale, preventing log error\n",
    "    df_clean['dist1'] = np.log(df_clean['dist1']+1)\n",
    "    df_clean['dist2'] = np.log(df_clean['dist2']+1)\n",
    "    df_clean['TransactionAmt'] = np.log(df_clean['TransactionAmt']+1)\n",
    "\n",
    "    cols_1 = [c for c in df_clean.columns if c in ['dist1','dist2','TransactionAmt']]\n",
    "    cols_2 = [c for c in df_clean.columns if c not in cols_1]\n",
    "\n",
    "    rob_scaler = RobustScaler(with_scaling=True, with_centering=False)\n",
    "    df_clean_rob = \\\n",
    "        pd.DataFrame(data=rob_scaler.fit_transform(df_clean[cols_1]), columns=['dist1','dist2','TransactionAmt'])\n",
    "\n",
    "    # Set the index of the scaled dataset. It is the same as the original dataset\n",
    "    s = df_clean.index\n",
    "    df_clean_rob = df_clean_rob.set_index([s])\n",
    "\n",
    "    #Merge the scaled dataset with the categorical features and the cleaned dataset with scaled numerical columns\n",
    "    df_clean_rob = \\\n",
    "        pd.merge(df_clean_rob, df_clean[cols_2],left_index=True, right_index=True)\n",
    "\n",
    "    #Check dimensions.\n",
    "    print(df_clean.shape)\n",
    "    print(df_clean_rob.shape)\n",
    "    \n",
    "    return df_clean_rob\n",
    "train_clean_rob = feature_scaling(train_clean)\n",
    "test_clean_rob  = feature_scaling(test_clean)\n",
    "\n",
    "del train_clean, test_clean\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n",
    "ax = sns.distplot(train_clean_rob['dist1'], bins=10, hist=False, ax=ax1)\n",
    "ax1.set_title('dist1 Distribution After Scaling', fontsize=14)\n",
    "\n",
    "ax = sns.distplot(train_clean_rob['dist2'], bins=10, hist=False, ax=ax2)\n",
    "ax2.set_title('dist2 Distribution After Scaling', fontsize=14)\n",
    "\n",
    "ax = sns.distplot(train_clean_rob['TransactionAmt'], bins=2, hist=False, ax=ax3)\n",
    "ax3.set_title('TransactionAmt Distribution After Scaling', fontsize=14)\n",
    "# train_clean_rob.head()\n",
    "# train_clean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction amount comparison fraud and non-fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,(ax1,ax2) = plt.subplots(2,1,sharex=True,figsize=(12,6))\n",
    "\n",
    "ax1.hist(train_clean_rob.TransactionAmt[train_clean_rob.isFraud==1],bins=30)\n",
    "ax1.set_title('Fraud')\n",
    "\n",
    "ax2.hist(train_clean_rob.TransactionAmt[train_clean_rob.isFraud==0],bins=30)\n",
    "ax2.set_title('Non-Fraud')\n",
    "\n",
    "plt.xlabel('TransactionAmt($)')\n",
    "plt.ylabel('Number of transaction')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of features between Fraud & non-Fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Distribution histogram of features\n",
    "# print(train_clean.columns)\n",
    "# features = train_clean.ix[:,4:5].columns;\n",
    "\n",
    "\n",
    "# for i,cn in enumerate(train_clean[features]):\n",
    "#     print(\"i: \",i)\n",
    "#     print(\"cn: \",cn)\n",
    "#     fig =  plt.figure(figsize=(20,12))\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     sns.distplot(train_clean[cn][train_clean.isFraud == 1], bins=50,color='r')\n",
    "#     sns.distplot(train_clean[cn][train_clean.isFraud == 0], bins=50,color='b')\n",
    "#     ax.set_xlabel('')\n",
    "#     ax.set_title('histogram of feature: ' + str(cn))\n",
    "#     plt.savefig('./output/pics/histgram of {}.png'.format(str(cn)))\n",
    "#     plt.clf()\n",
    "# #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Split for LogisticRegression()\n",
    "* Normal data is divided into for learning and testing, and for testing at a ratio of 70:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.3,shuffle = False )\n",
    "\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Split for AutoEncoder\n",
    "* Normal data is divided into for learning, testing, and testing at a ratio of 60:20:20, and abnormal data is not used for learning and is used only for testing and verification, so it divides into 50:50 ratio for testing and verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_clean_rob #memory ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split non_fraud by 60%,20%,20% (training, validation, test )\n",
    "\n",
    "df_nonfraud = train_clean_rob[train_clean_rob.isFraud == 0.0]\n",
    "df_fraud = train_clean_rob[train_clean_rob.isFraud == 1.0]\n",
    "\n",
    "#Non Fraud Datasets\n",
    "df_nonfraud_train,df_nonfraud_validate,df_nonfraud_test = \\\n",
    "    np.split(df_nonfraud,[int(0.6*len(df_nonfraud)),int(0.8*len(df_nonfraud))])\n",
    "\n",
    "#Fraud Datasets (Just for validation, test)\n",
    "df_fraud_validate,df_fraud_test = \\\n",
    "    np.split(df_fraud,[int(0.5*len(df_fraud))])\n",
    "\n",
    "del df_nonfraud, df_fraud\n",
    "gc.collect()\n",
    "\n",
    "#Merge datas and Shufflling for train\n",
    "df_train = df_nonfraud_train.sample(frac=1) \n",
    "df_validate = df_nonfraud_validate.append(df_fraud_validate).sample(frac=1)\n",
    "df_test = df_nonfraud_test.append(df_fraud_test).sample(frac=1)\n",
    "\n",
    "del df_nonfraud_train,df_nonfraud_validate,df_nonfraud_test,df_fraud_validate,df_fraud_test\n",
    "gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df[-1] == Fraud column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraud_label_mov(df): \n",
    "    cols = df.columns.tolist()\n",
    "    tmpcols = cols[3:4]\n",
    "    cols = cols[:3] + cols[4:] \n",
    "    cols = cols[:] + tmpcols\n",
    "    df= df[cols]\n",
    "    return df\n",
    "fraud_label_mov(df_train)\n",
    "fraud_label_mov(df_validate)\n",
    "fraud_label_mov(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "df_train2 = pd.get_dummies(df_train)\n",
    "reduce_mem_usage(df_train2)\n",
    "\n",
    "df_validate2 = pd.get_dummies(df_validate)\n",
    "reduce_mem_usage(df_validate2)\n",
    "\n",
    "df_test2 = pd.get_dummies(df_test)\n",
    "reduce_mem_usage(df_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.to_csv(\"./cleaned/cleaned_train.csv\", mode='w', header=False)\n",
    "df_validate2.to_csv(\"./cleaned/cleaned_validate.csv\", mode='w', header=False)\n",
    "df_test2.to_csv(\"./cleaned/cleaned_test.csv\", mode='w', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_train.iloc[:,388:389].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_with_Fraud = train.corrwith(train['isFraud'])\n",
    "# for d in corr_with_Fraud:\n",
    "#     if(d > 0.01):\n",
    "#         print((corr_with_Fraud==d).argmax(), d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_with_Fraud = train.corrwith(train['isFraud'])\n",
    "# print(corr_with_Fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X2= np.array(X)\n",
    "# y = np.array(y)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.3,shuffle = False )\n",
    "# print(X2.shape)\n",
    "# print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_train[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glm = LogisticRegression()\n",
    "# glm.fit(X_train, y_train[:])\n",
    "# y_pred = glm.predict(X_test)\n",
    "# acc = np.mean(y_test[:] == y_pred )\n",
    "# print(\"SKLEARN Logistic Regression Accuracy = {:3.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def split_value_label(filename_queue):\n",
    "def read_and_decode(filename_queue):\n",
    "    reader = tf.TextLineReader()\n",
    "    key,value = reader.read(filename_queue)\n",
    "    record_defaults = [ [0.0] ]*390\n",
    "    columns = tf.decode_csv(value,record_defaults = record_defaults)\n",
    "    # first column is time field from 1 to 28 column is feature, 2\n",
    "    value = tf.convert_to_tensor(columns[1:388],dtype=tf.float32)\n",
    "    value.set_shape([387])\n",
    "    label =tf.cast( columns[388],tf.int32)\n",
    "    \n",
    "    return value,label\n",
    "\n",
    "def input_fn(filename,batch_size=100):\n",
    "    filename_queue = tf.train.string_input_producer([filename])\n",
    "    \n",
    "    value,label = read_and_decode(filename_queue)\n",
    "    values,labels = tf.train.batch(\n",
    "        [value,label],batch_size=batch_size,\n",
    "        capacity = 1000+3*batch_size)\n",
    "    return {'inputs':values},labels\n",
    "\n",
    "def get_input_fn(filename,batch_size=100):\n",
    "    return lambda: input_fn(filename,batch_size)\n",
    "\n",
    "def serving_input_fn():\n",
    "    inputs = {'inputs':tf.placeholder(tf.float32,[None,387])}\n",
    "    return tf.estimator.export.ServingInputReceiver(inputs,inputs)\n",
    "\n",
    "def autoencoder_model_fn(features,labels,mode):\n",
    "    input_layer = features['inputs']\n",
    "    dense1 = tf.layers.dense(inputs=input_layer,units=380,activation=tf.nn.relu)\n",
    "    dense2 = tf.layers.dense(inputs=dense1,units=370,activation=tf.nn.relu)\n",
    "    dense3 = tf.layers.dense(inputs=dense2,units=360,activation=tf.nn.relu)\n",
    "    dense4 = tf.layers.dense(inputs=dense3,units=370,activation=tf.nn.relu)\n",
    "    dense5 = tf.layers.dense(inputs=dense4,units=380,activation=tf.nn.relu)\n",
    "    output_layer = tf.layers.dense(inputs=dense5,units=387,activation=tf.nn.sigmoid)\n",
    "    \n",
    "    #training and evaluation mode\n",
    "    if mode in (Modes.TRAIN,Modes.EVAL):\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        label_indices = tf.cast(labels,tf.int32)\n",
    "        loss = tf.reduce_sum(tf.square(output_layer - input_layer))\n",
    "        tf.summary.scalar('OptimizeLoss',loss)\n",
    "\n",
    "        if mode == Modes.TRAIN:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "            train_op = optimizer.minimize(loss,global_step=global_step)\n",
    "            return tf.estimator.EstimatorSpec(mode,loss = loss, train_op = train_op)\n",
    "        if mode == Modes.EVAL:\n",
    "            eval_metric_ops = None\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode,loss=loss,eval_metric_ops = eval_metric_ops)\n",
    "        \n",
    "    # prediction mode\n",
    "    if mode == Modes.PREDICT:\n",
    "        predictions={\n",
    "            'outputs':output_layer\n",
    "        }\n",
    "        export_outputs={\n",
    "            'outputs':tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,predictions=predictions,export_outputs=export_outputs)\n",
    "\n",
    "def build_estimator(model_dir):\n",
    "    return tf.estimator.Estimator(\n",
    "        model_fn = autoencoder_model_fn,\n",
    "        model_dir = model_dir,\n",
    "        config=tf.contrib.learn.RunConfig(save_checkpoints_secs=180))\n",
    "\n",
    "def generate_experiment_fn(data_dir,\n",
    "                          train_batch_size = 100,\n",
    "                          eval_batch_size = 100,\n",
    "                          train_steps = 20000,\n",
    "                          eval_steps = 1,\n",
    "                          **experiment_args):\n",
    "    def _experiment_fn(output_dir):\n",
    "        return Experiment(\n",
    "            build_estimator(output_dir),\n",
    "            train_input_fn=get_input_fn('./cleaned/cleaned_train.csv',batch_size=train_batch_size),\n",
    "            eval_input_fn=get_input_fn('./cleaned/cleaned_test.csv',batch_size=eval_batch_size),\n",
    "            export_strategies = [saved_model_export_utils.make_export_strategy(\n",
    "                serving_input_fn,\n",
    "                default_output_alternative_key=None,\n",
    "                exports_to_keep=1)\n",
    "            ],\n",
    "            train_steps = train_steps,\n",
    "            eval_steps = eval_steps,\n",
    "            **experiment_args\n",
    "        )\n",
    "    return _experiment_fn\n",
    "\n",
    "shutil.rmtree(OUTDIR, ignore_errors=True) # start fresh each time\n",
    "learn_runner.run(\n",
    "    generate_experiment_fn(\n",
    "        data_dir='./cleaned/',\n",
    "        train_steps=5000),OUTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[tensorflow]",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
