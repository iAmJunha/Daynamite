{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FDS (Fraud Detection System, 이상금융거래탐지시스템)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing\n",
    "import missingno as msno\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.preprocessing import Imputer, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reduce Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed load\n",
      "Completed load\n",
      "Completed load\n",
      "Completed load\n",
      "Completed load\n"
     ]
    }
   ],
   "source": [
    "train_tr = pd.read_csv('./input/train_transaction.csv', index_col='TransactionID')\n",
    "print(\"Completed load\")\n",
    "train_id = pd.read_csv('./input/train_identity.csv', index_col='TransactionID')\n",
    "print(\"Completed load\")\n",
    "test_tr = pd.read_csv('./input/test_transaction.csv', index_col='TransactionID')\n",
    "print(\"Completed load\")\n",
    "test_id = pd.read_csv('./input/test_identity.csv', index_col='TransactionID')\n",
    "print(\"Completed load\")\n",
    "sub = pd.read_csv('./input/sample_submission.csv', index_col='TransactionID')\n",
    "print(\"Completed load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Left join by TransactionId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train_tr, train_id,\n",
    "                        how='left',\n",
    "                        on='TransactionID')\n",
    "del train_tr, train_id\n",
    "\n",
    "test = pd.merge(test_tr, test_id,\n",
    "                        how='left',\n",
    "                        on='TransactionID')\n",
    "del test_tr, test_id\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test  = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EDA(Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert to object for reuse\n",
    "train['ProductCD'] = train['ProductCD'].astype('object')\n",
    "train['P_emaildomain'] = train['P_emaildomain'].astype('object')\n",
    "train['R_emaildomain'] = train['R_emaildomain'].astype('object')    \n",
    "train['DeviceType'] = train['DeviceType'].astype('object')\n",
    "train['DeviceInfo'] = train['DeviceInfo'].astype('object')\n",
    "\n",
    "card_cols = [c for c in train.columns if 'card' in c]\n",
    "for col in card_cols:\n",
    "    train[col] = train[col].astype('object')\n",
    "\n",
    "addres_cols = [c for c in train.columns if 'addr' in c]\n",
    "for col in addres_cols:\n",
    "        train[col] = train[col].astype('object')\n",
    "\n",
    "M_cols = [c for c in train.columns if 'M' in c]\n",
    "for col in M_cols:\n",
    "        train[col] = train[col].astype('object')\n",
    "        \n",
    "C_cols = [c for c in train.columns if 'C' in c]\n",
    "for col in C_cols:\n",
    "        train[col] = train[col].astype('object')\n",
    "        \n",
    "id_cols = [c for c in train.columns if 'id' in c]\n",
    "for col in id_cols:\n",
    "        train[col] = train[col].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n",
    "ax = sns.distplot(train['dist1'].dropna(axis = 0), bins=10, hist=False, ax=ax1)\n",
    "ax1.set_title('dist1 Distribution', fontsize=14)\n",
    "\n",
    "ax = sns.distplot(train['dist2'].dropna(axis = 0), bins=10, hist=False, ax=ax2)\n",
    "ax2.set_title('dist2 Distribution', fontsize=14)\n",
    "\n",
    "ax = sns.distplot(train['TransactionAmt'].dropna(axis = 0), bins=2, hist=False, ax=ax3)\n",
    "ax3.set_title('TransactionAmt Distribution', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_statistics(df):\n",
    "    missing_values_count = df[df.columns].isnull().sum()\n",
    "    print (missing_values_count.head())\n",
    "    total_cells = np.product(df.shape)\n",
    "    total_missing = missing_values_count.sum()\n",
    "    print (\"% of missing data = \",(total_missing/total_cells) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_categorical(df, threshold=100.00): \n",
    "    missing_values_count = df.isnull().sum()\n",
    "    missing_percentage= (missing_values_count/df.shape[0]) * 100\n",
    "    columns = missing_percentage[missing_percentage<threshold].index\n",
    "    df = df[columns]\n",
    "    \n",
    "    features_categorical = df.select_dtypes(include = ['object']).columns\n",
    "    df[features_categorical] = df[features_categorical].fillna('-999',inplace=False)\n",
    "    \n",
    "    missing_value_statistics(df)\n",
    "    \n",
    "    del features_categorical\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_numerical(df, threshold=100.00): \n",
    "    missing_values_count = df.isnull().sum()\n",
    "    missing_percentage= (missing_values_count/df.shape[0]) * 100\n",
    "    columns = missing_percentage[missing_percentage<threshold].index\n",
    "    df = df[columns]\n",
    "    \n",
    "    features_dist_transAmt = df[['dist1','dist2','TransactionAmt']].columns\n",
    "    \n",
    "    imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    imputer = imputer.fit(df[features_dist_transAmt])\n",
    "    \n",
    "    df = imputer.transform(df[features_dist_transAmt])\n",
    "    df = pd.DataFrame(df, columns=['dist1','dist2','TransactionAmt'])\n",
    "    \n",
    "    del features_dist_transAmt\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"% of train data missing = \",(train[train.columns].isnull().sum().sum()/np.product(train.shape)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=100.0\n",
    "features_numeric = train.select_dtypes(include = ['int32','int8','float16','float64', 'int64']).columns\n",
    "col_1 = [c for c in features_numeric if c in ['dist1','dist2','TransactionAmt']]\n",
    "col_2 = [c for c in features_numeric if c not in col_1]\n",
    "\n",
    "s=train.index\n",
    "train_clean_col_1 = missing_values_numerical(train[col_1], threshold)\n",
    "train_clean_col_1 = train_clean_col_1.set_index([s])\n",
    "\n",
    "train_clean_col_2 = train[col_2].fillna(-999)\n",
    "train_clean_col_2 = train_clean_col_2.set_index([s])\n",
    "\n",
    "train_clean_N = train_clean_col_1.merge(train_clean_col_2, left_on='TransactionID', right_on='TransactionID', how='left')\n",
    "missing_value_statistics(train_clean_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=100.0\n",
    "features_category = train.select_dtypes(include = ['object']).columns\n",
    "train_clean_C = missing_values_categorical(train[features_category], threshold)\n",
    "train_clean_C['card1'] = train_clean_C['card1'].astype('object')\n",
    "missing_value_statistics(train_clean_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = train_clean_N.merge(train_clean_C, left_on='TransactionID', right_on='TransactionID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train, train_clean_N, train_clean_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransactionDT guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '2018-01-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "train_clean['TransactionDT'] = train_clean['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "print(train_clean.head())\n",
    "print(train_clean.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n",
    "ax = sns.distplot(train_clean['dist1'].dropna(axis = 0), bins=10, hist=False, ax=ax1)\n",
    "ax1.set_title('dist1 Distribution', fontsize=14)\n",
    "\n",
    "ax = sns.distplot(train_clean['dist2'].dropna(axis = 0), bins=10, hist=False, ax=ax2)\n",
    "ax2.set_title('dist2 Distribution', fontsize=14)\n",
    "\n",
    "ax = sns.distplot(train_clean['TransactionAmt'].dropna(axis = 0), bins=2, hist=False, ax=ax3)\n",
    "ax3.set_title('TransactionAmt Distribution', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean['TransactionAmt'] = np.log(train_clean['TransactionAmt']+1)\n",
    "train_clean['dist1'] = np.log(train_clean['dist1']+1)\n",
    "train_clean['dist2'] = np.log(train_clean['dist2']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체 스케일링 결과는 비슷하지만 아웃라이어를 제거한 \n",
    "#나머지 데이터의 분포는 로버스트 스케일링을 사용했을 때가 더 좋다.\n",
    "#아웃라이어 검출이 목적이므로 가장 좋은 스케일링 방법이라 볼 수 있다.\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "cols_1 = [c for c in train_clean.columns if c in ['dist1','dist2','TransactionAmt']]\n",
    "cols_2 = [c for c in train_clean.columns if c not in col_1]\n",
    "\n",
    "# RobustScaler is less prone to outliers.\n",
    "rob_scaler = RobustScaler(with_scaling=True, with_centering=False)\n",
    "train_clean_rob = pd.DataFrame(data=rob_scaler.fit_transform(train_clean[cols_1]), columns=['dist1','dist2','TransactionAmt'])\n",
    "\n",
    "# Set the index of the scaled dataset. It is the same as the original dataset\n",
    "s=train_clean.index\n",
    "train_clean_rob = train_clean_rob.set_index([s])\n",
    "\n",
    "#Merge the scaled dataset with the categorical features and the [\"isFraud\", \"TransactionDT\"] columns to get back the cleaned \n",
    "#dataset but with scaled numerical columns\n",
    "train_clean_rob = pd.merge(train_clean_rob, train_clean[cols_2],left_index=True, right_index=True)\n",
    "\n",
    "#Just a check of the dimensions.\n",
    "print(train_clean.shape)\n",
    "print(train_clean_rob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n",
    "ax = sns.distplot(train_clean_rob['dist1'], bins=10, hist=False, ax=ax1)\n",
    "ax1.set_title('dist1 Distribution After Scaling', fontsize=14)\n",
    "\n",
    "ax = sns.distplot(train_clean_rob['dist2'], bins=10, hist=False, ax=ax2)\n",
    "ax2.set_title('dist2 Distribution After Scaling', fontsize=14)\n",
    "\n",
    "ax = sns.distplot(train_clean_rob['TransactionAmt'], bins=2, hist=False, ax=ax3)\n",
    "ax3.set_title('TransactionAmt Distribution After Scaling', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_clean_rob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_clean_rob['TransactionDT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction amount comparison fraud and non-fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,(ax1,ax2) = plt.subplots(2,1,sharex=True,figsize=(12,6))\n",
    "bins = 30\n",
    "\n",
    "ax1.hist(train_clean_rob.TransactionAmt[train_clean_rob.isFraud==1],bins=bins)\n",
    "ax1.set_title('Fraud')\n",
    "\n",
    "ax2.hist(train_clean_rob.TransactionAmt[train_clean_rob.isFraud==0],bins=bins)\n",
    "ax2.set_title('Non-Fraud')\n",
    "\n",
    "plt.xlabel('TransactionAmt($)')\n",
    "plt.ylabel('Number of transaction')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "features = train_clean.ix[:,51:100].columns;\n",
    "\n",
    "\n",
    "for i,cn in enumerate(train_clean[features]):\n",
    "    print(\"i: \",i)\n",
    "    print(\"cn: \",cn)\n",
    "    fig =  plt.figure(figsize=(20,12))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.distplot(train_clean[cn][train_clean.isFraud == 1], bins=50,color='r')\n",
    "    sns.distplot(train_clean[cn][train_clean.isFraud == 0], bins=50,color='b')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title('histogram of feature: ' + str(cn))\n",
    "    plt.savefig('./output/pics/histgram of {}.png'.format(str(cn)))\n",
    "    plt.clf()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Min-Max NOrmarlization\n",
    "# from sklearn.preprocessing import minmax_scale, StandardScaler, MinMaxScaler\n",
    "# df['A'].apply(minmax_scale)\n",
    "\n",
    "# minmax_scale = MinMaxScaler(feature_range=[0,1]).fit(df[['A', 'B']]) # A, B 컬럼 각각 standard_scaler가 만들어짐\n",
    "# df_minmax = minmax_scale.transform(df[['A', 'B']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['A'].apply(lambda x: StandardScaler(x))\n",
    "# std_scale = StandardScaler().fit(df[['A', 'B']]) # A, B 컬럼 각각 standard_scaler가 만들어짐\n",
    "# df_std = std_scale.transform(df[['A', 'B']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train.isnull().sum(axis=0)\n",
    "# #train.isnull().mean(axis=0)\n",
    "# #train['dist1'].isnull().mean(axis=0)\n",
    "# # train.dropna(axis=0, thresh= 3)\n",
    "# # train.fillna(value=0)\n",
    "\n",
    "# def Preprocess_rate(targetdf, cols, drop_rate):\n",
    "#     df = targetdf.copy()\n",
    "#     for col in cols:\n",
    "#         if(df[col].isnull().mean(axis=0) > drop_rate):\n",
    "#             df = df.drop(columns = col)\n",
    "#     return df\n",
    "# newdf = Preprocess_rate(train, train.columns, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(newdf.C1.dropna())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fill NaN by most_frequency (have to adapt with categorical columns)\n",
    "# newdf2 = pd.get_dummies(newdf)\n",
    "# def Most_freq(targetdf, cols):\n",
    "#     df = targetdf.copy()\n",
    "#     for col in cols:\n",
    "#         if(df[col].isnull().mean(axis=0) > 0):\n",
    "#             print(col)\n",
    "#             most_frequency = df[col].value_counts(dropna = True).idxmax()\n",
    "#             print(most_frequency)\n",
    "#             df = df[col].fillna(most_frequency, inplace = True)\n",
    "#     return df\n",
    "\n",
    "# newdf3 = Most_freq(newdf2, newdf2.columns)\n",
    "# newdf3.isnull().mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_frequency = newdf2['TransactionDT'].value_counts(dropna = True).idxmax()\n",
    "# print(most_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(frauddf.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_with_Fraud = train.corrwith(train['isFraud'])\n",
    "# for d in corr_with_Fraud:\n",
    "#     if(d > 0.01):\n",
    "#         print((corr_with_Fraud==d).argmax(), d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_with_Fraud = train.corrwith(train['isFraud'])\n",
    "# print(corr_with_Fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msno.heatmap(train.iloc[:, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msno.matrix(df=train.iloc[:, 61:80], figsize=(20, 20), color=(0.2, 0.3, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###Fraud ratio pie chart\n",
    "# f, ax = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "# train['isFraud'].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0], shadow=True)\n",
    "# ax[0].set_title('Pie plot - isFraud')\n",
    "# ax[0].set_ylabel('')\n",
    "# sns.countplot('isFraud', data=train, ax=ax[1])\n",
    "# ax[1].set_title('Count plot - isFraud')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['isFraud'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_tr['isFraud'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #without preprocessing\n",
    "\n",
    "# # del train\n",
    "# # del train_tr\n",
    "# # del train_id\n",
    "\n",
    "# # train = train.drop(columns=['dist2','R_emaildomain','D2','D3','D4','D5',\n",
    "# #                              'D7','D8','D9','D12','D13','D14'\n",
    "# #                             ])\n",
    "# # train = train.drop(columns=['V1','V2'])\n",
    "# train = train.loc[:,['isFraud','TransactionDT','TransactionAmt','ProductCD','card1',\n",
    "#                      'card2','card3','C1','C2','C3','C4',\n",
    "#                      'V12'\n",
    "#                     ]]\n",
    "# #print(train)\n",
    "# train2 = pd.get_dummies(train)\n",
    "# train2=train2.dropna(axis=0)\n",
    "# #train2.isnull().sum(axis=0)\n",
    "\n",
    "# #y= pd.get_dummies(train2.isFraud)\n",
    "# y =train2.isFraud\n",
    "# X = train2.drop(columns=['isFraud'])\n",
    "\n",
    "# #headerX = train2.drop(columns=['isFraud']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X2= np.array(X)\n",
    "# y = np.array(y)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.3,shuffle = False )\n",
    "# print(X2.shape)\n",
    "# print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_train[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glm = LogisticRegression()\n",
    "# glm.fit(X_train, y_train[:])\n",
    "# y_pred = glm.predict(X_test)\n",
    "# acc = np.mean(y_test[:] == y_pred )\n",
    "# print(\"SKLEARN Logistic Regression Accuracy = {:3.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[tensorflow]",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
